{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMZKYRfNwcTPoXc/2d0+k3z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/slimshadow-git/liteapks-scraper/blob/main/lite_apks_scraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This is the tutorial on how to scrape liteapks website- for future use(update and upgrade) by slim shadow"
      ],
      "metadata": {
        "id": "mTacsJYjwBj5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What we do We will be scraping the lite apks pages along with all app list using this code: this will create website_list.txt file which contain all the files"
      ],
      "metadata": {
        "id": "lnHvZHNzwOZK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests beautifulsoup4"
      ],
      "metadata": {
        "id": "4qRaovuE5CO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Base URL structure\n",
        "base_url = \"http://liteapks.com/apps/page/\"\n",
        "\n",
        "# Range of pages to scrape (update this range based on your requirements)\n",
        "start_page = 1\n",
        "end_page = 185  # Change this to the desired number of pages\n",
        "\n",
        "# List to store all scraped links\n",
        "all_links = []\n",
        "\n",
        "try:\n",
        "    # Loop through all pages\n",
        "    for page_num in range(start_page, end_page + 1):\n",
        "        url = f\"{base_url}{page_num}\"  # Generate the page URL\n",
        "        print(f\"Scraping: {url}\")\n",
        "\n",
        "        # Send a GET request to fetch the content of the current page\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()  # Raise an exception for HTTP errors\n",
        "\n",
        "        # Parse the content with BeautifulSoup\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Find all <div> elements with class \"row\"\n",
        "        row_divs = soup.find_all('div', class_='row')\n",
        "\n",
        "        # Extract all <a> tags with href attributes inside those <div> elements\n",
        "        for div in row_divs:\n",
        "            links = [a['href'] for a in div.find_all('a', href=True)]\n",
        "            all_links.extend(links)  # Add these links to the main list\n",
        "\n",
        "    # Save the links to a file\n",
        "    with open(\"scraped.txt\", \"w\") as file:\n",
        "        for link in all_links:\n",
        "            file.write(link + \"\\n\")  # Write each link on a new line\n",
        "\n",
        "    print(f\"\\nScraped {len(all_links)} links and saved them to 'scraped.txt'.\")\n",
        "\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Error fetching the URL: {e}\")\n"
      ],
      "metadata": {
        "id": "lu6HMDU7sNZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will use this code to scrape download button link which will take website one step further and create a download_button_links.txt"
      ],
      "metadata": {
        "id": "yPlieS52wWWX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from tqdm import tqdm  # For progress bar\n",
        "\n",
        "# Function to scrape a single website\n",
        "def scrape_website(session, website):\n",
        "    try:\n",
        "        response = session.get(website, timeout=10)  # Set a timeout for faster failure\n",
        "        response.raise_for_status()  # Raise an error for bad responses\n",
        "        soup = BeautifulSoup(response.text, 'html.parser  ')  # Use the faster lxml parser\n",
        "        links = soup.find_all('a', class_='btn btn-primary btn-block mb-4')\n",
        "        return [link.get('href') for link in links if link.get('href')]\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching {website}: {e}\")\n",
        "        return []\n",
        "\n",
        "# Read the list of websites from the file\n",
        "with open('a.txt', 'r') as file:\n",
        "    websites = [line.strip() for line in file.readlines()]\n",
        "\n",
        "# Start the timer\n",
        "start_time = time.time()\n",
        "\n",
        "# Use ThreadPoolExecutor for concurrent scraping\n",
        "with ThreadPoolExecutor(max_workers=20) as executor:  # Adjust max_workers based on your CPU\n",
        "    with requests.Session() as session:  # Reuse session for connection pooling\n",
        "        # Submit all tasks to the executor\n",
        "        futures = {executor.submit(scrape_website, session, website): website for website in websites}\n",
        "\n",
        "        # Open the file in append mode\n",
        "        with open('scraped_links.txt', 'a') as output_file:\n",
        "            # Use tqdm to display progress\n",
        "            for future in tqdm(as_completed(futures), total=len(futures), desc=\"Scraping websites\"):\n",
        "                links = future.result()\n",
        "\n",
        "                # Write links to the file immediately\n",
        "                if links:\n",
        "                    output_file.write('\\n'.join(links) + '\\n')\n",
        "\n",
        "# Final output\n",
        "total_time = time.time() - start_time\n",
        "print(f\"\\nScraping completed. Links saved to scraped_links.txt.\")\n",
        "print(f\"Total time taken: {total_time:.2f} seconds.\")\n"
      ],
      "metadata": {
        "id": "Q71Kqj3CvIhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code will generate links that ends with file versions"
      ],
      "metadata": {
        "id": "qXJY4JmBzsmk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Open the input file and read all lines\n",
        "with open('scraped_links.txt', 'r') as file:\n",
        "    links = [line.strip() for line in file.readlines()]  # Strip newline characters\n",
        "\n",
        "# Open the output file in write mode\n",
        "with open('second_step.txt', 'w') as output_file:\n",
        "    # Loop through each link and generate the modified links\n",
        "    for link in links:\n",
        "        for i in range(1, 5):  # Generate /1, /2, /3, /4\n",
        "            modified_link = f\"{link}/{i}\"  # Append the suffix\n",
        "            output_file.write(modified_link + '\\n')  # Write to the output file\n",
        "\n",
        "print(\"Links have been modified and saved to second_step.txt.\")"
      ],
      "metadata": {
        "id": "oLRd9doWzaru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now the step 4:"
      ],
      "metadata": {
        "id": "Mkb4t6o24ox5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Function to scrape links and filenames from a given URL\n",
        "def scrape_links_and_filenames(url):\n",
        "    try:\n",
        "        # Send a GET request to fetch the content\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()  # Raise an exception for HTTP errors\n",
        "\n",
        "        # Parse the content with BeautifulSoup\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Find all <a> tags with href attributes\n",
        "        links = soup.find_all('a', href=True)\n",
        "\n",
        "        # Filter links that start with \"https://s1.spiderdown.com/\"\n",
        "        spiderdown_links = [link['href'] for link in links if link['href'].startswith(\"https://s1.spiderdown.com/\")]\n",
        "\n",
        "        # Find the filename from the <h1> tag\n",
        "        filename_tag = soup.find('h1', class_=\"h5 font-weight-semibold mb-3\")\n",
        "        filename = filename_tag.text.strip() if filename_tag else \"Unknown Filename\"\n",
        "\n",
        "        return spiderdown_links, filename\n",
        "\n",
        "    except requests.exceptions.RequestException:\n",
        "        return [], \"Error\"\n",
        "\n",
        "# Read URLs from second_step.txt\n",
        "with open('second_step.txt', 'r') as file:\n",
        "    urls = file.readlines()\n",
        "\n",
        "# Open final_step.txt to write the results\n",
        "with open('final_step.txt', 'w') as output_file:\n",
        "    for index, url in enumerate(urls):\n",
        "        url = url.strip()  # Remove any leading/trailing whitespace\n",
        "        if url:  # Check if the URL is not empty\n",
        "            print(f\"Processing URL {index + 1}/{len(urls)}\")\n",
        "            links, filename = scrape_links_and_filenames(url)\n",
        "            for link in links:\n",
        "                # Write the filename and link to the file\n",
        "                output_file.write(f\"{filename}: {link}\\n\")\n",
        "\n",
        "print(\"Scraping completed. Check final_step.txt for results.\")\n"
      ],
      "metadata": {
        "id": "R_8IJTF54MQC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}